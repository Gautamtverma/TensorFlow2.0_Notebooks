{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier\n",
    "\n",
    "#### Author: Ivan Bongiorni, Data Scientist at GfK.\n",
    "\n",
    "[LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/).\n",
    "\n",
    "This model is a **feed forward Neural Network Classifier** based on the University of Wisconsin's **breast cancer dataset**.\n",
    "\n",
    "It represents a tutorial on basic TensorFlow 2.0 (alpha). The 1.x version of the same model can be found in [this Notebook](https://github.com/IvanBongiorni/TensorFlow_Tutorial/blob/master/TensorFlow_1_Classification_BatchGD.ipynb).\n",
    "\n",
    "Summary:\n",
    "\n",
    "1. Import data + quick dataprep,\n",
    "2. Neural Network architecture,\n",
    "3. Implementation of full-Batch Gradient Descent,\n",
    "4. Visualization,\n",
    "5. Wrap the Trainin process in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None  # this prints all columns\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data + dataprep\n",
    "\n",
    "The purpose of this dataset is to classify breast cancer cases between **malignant** (M), and **benign** (B). Therefore, my classification network will have two output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from the UCI ML repository\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", \n",
    "                 sep=\",\", header=None)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9       10       11      12      13     14      15        16       17  \\\n",
       "0  0.14710  0.2419  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904   \n",
       "1  0.07017  0.1812  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308   \n",
       "2  0.12790  0.2069  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006   \n",
       "3  0.10520  0.2597  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458   \n",
       "4  0.10430  0.1809  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461   \n",
       "\n",
       "        18       19       20        21     22     23      24      25      26  \\\n",
       "0  0.05373  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622   \n",
       "1  0.01860  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238   \n",
       "2  0.03832  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444   \n",
       "3  0.05661  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098   \n",
       "4  0.05688  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374   \n",
       "\n",
       "       27      28      29      30       31  \n",
       "0  0.6656  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.1866  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.4245  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.8663  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.2050  0.4000  0.1625  0.2364  0.07678  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>4.885000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0           2           3           4            5   \\\n",
       "count  5.690000e+02  569.000000  569.000000  569.000000   569.000000   \n",
       "mean   3.037183e+07   14.127292   19.289649   91.969033   654.889104   \n",
       "std    1.250206e+08    3.524049    4.301036   24.298981   351.914129   \n",
       "min    8.670000e+03    6.981000    9.710000   43.790000   143.500000   \n",
       "25%    8.692180e+05   11.700000   16.170000   75.170000   420.300000   \n",
       "50%    9.060240e+05   13.370000   18.840000   86.240000   551.100000   \n",
       "75%    8.813129e+06   15.780000   21.800000  104.100000   782.700000   \n",
       "max    9.113205e+08   28.110000   39.280000  188.500000  2501.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.096360    0.104341    0.088799    0.048919    0.181162    0.062798   \n",
       "std      0.014064    0.052813    0.079720    0.038803    0.027414    0.007060   \n",
       "min      0.052630    0.019380    0.000000    0.000000    0.106000    0.049960   \n",
       "25%      0.086370    0.064920    0.029560    0.020310    0.161900    0.057700   \n",
       "50%      0.095870    0.092630    0.061540    0.033500    0.179200    0.061540   \n",
       "75%      0.105300    0.130400    0.130700    0.074000    0.195700    0.066120   \n",
       "max      0.163400    0.345400    0.426800    0.201200    0.304000    0.097440   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.405172    1.216853    2.866059   40.337079    0.007041    0.025478   \n",
       "std      0.277313    0.551648    2.021855   45.491006    0.003003    0.017908   \n",
       "min      0.111500    0.360200    0.757000    6.802000    0.001713    0.002252   \n",
       "25%      0.232400    0.833900    1.606000   17.850000    0.005169    0.013080   \n",
       "50%      0.324200    1.108000    2.287000   24.530000    0.006380    0.020450   \n",
       "75%      0.478900    1.474000    3.357000   45.190000    0.008146    0.032450   \n",
       "max      2.873000    4.885000   21.980000  542.200000    0.031130    0.135400   \n",
       "\n",
       "               18          19          20          21          22          23  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.031894    0.011796    0.020542    0.003795   16.269190   25.677223   \n",
       "std      0.030186    0.006170    0.008266    0.002646    4.833242    6.146258   \n",
       "min      0.000000    0.000000    0.007882    0.000895    7.930000   12.020000   \n",
       "25%      0.015090    0.007638    0.015160    0.002248   13.010000   21.080000   \n",
       "50%      0.025890    0.010930    0.018730    0.003187   14.970000   25.410000   \n",
       "75%      0.042050    0.014710    0.023480    0.004558   18.790000   29.720000   \n",
       "max      0.396000    0.052790    0.078950    0.029840   36.040000   49.540000   \n",
       "\n",
       "               24           25          26          27          28  \\\n",
       "count  569.000000   569.000000  569.000000  569.000000  569.000000   \n",
       "mean   107.261213   880.583128    0.132369    0.254265    0.272188   \n",
       "std     33.602542   569.356993    0.022832    0.157336    0.208624   \n",
       "min     50.410000   185.200000    0.071170    0.027290    0.000000   \n",
       "25%     84.110000   515.300000    0.116600    0.147200    0.114500   \n",
       "50%     97.660000   686.500000    0.131300    0.211900    0.226700   \n",
       "75%    125.400000  1084.000000    0.146000    0.339100    0.382900   \n",
       "max    251.200000  4254.000000    0.222600    1.058000    1.252000   \n",
       "\n",
       "               29          30          31  \n",
       "count  569.000000  569.000000  569.000000  \n",
       "mean     0.114606    0.290076    0.083946  \n",
       "std      0.065732    0.061867    0.018061  \n",
       "min      0.000000    0.156500    0.055040  \n",
       "25%      0.064930    0.250400    0.071460  \n",
       "50%      0.099930    0.282200    0.080040  \n",
       "75%      0.161400    0.317900    0.092080  \n",
       "max      0.291000    0.663800    0.207500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the target variable\n",
    "\n",
    "Let's explore the dependent variable. Its a string column, referring to the 'M' and 'B' targt categories. Before feeding the data in a Neural Network, it will require one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M', 'B'}\n",
      "no. classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(set(df[df.columns[1]]))\n",
    "print('no. classes: ' + str(len(set(df[df.columns[1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "classification = pd.get_dummies(df[df.columns[1]])\n",
    "print(classification.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained a target dataframe, called `classification`, that contains the one-hot encoded version of my dependent binary variable. Now I can isolate the explanatory variables in my dataframe `df`. In order to do that I'll drop columns 0 and 1: the first is an index, the second is the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      2      3       4       5        6        7       8        9       10  \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        11      12      13     14      15        16       17       18  \\\n",
       "0  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904  0.05373   \n",
       "1  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308  0.01860   \n",
       "2  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006  0.03832   \n",
       "3  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458  0.05661   \n",
       "4  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461  0.05688   \n",
       "\n",
       "        19       20        21     22     23      24      25      26      27  \\\n",
       "0  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622  0.6656   \n",
       "1  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238  0.1866   \n",
       "2  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444  0.4245   \n",
       "3  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098  0.8663   \n",
       "4  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374  0.2050   \n",
       "\n",
       "       28      29      30       31  \n",
       "0  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.4000  0.1625  0.2364  0.07678  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([df.columns[0], df.columns[1]], axis=1)   # drop the target variable from df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the shape is:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, I turn both explanatory and target data into numpy objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.values\n",
    "classification = classification.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, I uniform the datatypes to float64\n",
    "classification = classification.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "\n",
    "In an actual ML job, you would split your dataset in **Train**, **Validation** and **Test**sets. However, this is just an example on how to implement and run a Neural Network, so I'll skip that part and will split the data in train and test only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (426, 30)\n",
      "y_train shape: (426, 2)\n",
      "\n",
      "X_test shape: (143, 30)\n",
      "y_test shape: (143, 2)\n"
     ]
    }
   ],
   "source": [
    "## TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, classification, test_size=0.25, random_state=173)\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape) + '\\ny_train shape: ' + str(y_train.shape))\n",
    "print('\\nX_test shape: ' + str(X_test.shape) + '\\ny_test shape: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the variables must happen after the train-test split. That is because the test set must be scaled using the parameters of the training set: in real world cases you don't know what data you'll get from training, therefore this is the only way to truly understand the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the variables using Z-scores\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network architecture\n",
    "\n",
    "Since the network is not very deep, and the number of parameters is relatively small, I can employ more \"demanding\" (and performing) activation functions. In this case, I choose **ELU** (**Exponential Linear Unit**) activations. A **softmax** function is then applied at the end, so that the attribution of classes (M/B) is shrinked into probabilities.\n",
    "\n",
    "Additionally, I apply **dropout** in order to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.activations import elu, softmax\n",
    "\n",
    "# Architecture\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden1 = 30\n",
    "n_hidden2 = 20\n",
    "n_hidden3 = 15\n",
    "n_output = y_train.shape[1]\n",
    "\n",
    "# set dropout probability\n",
    "dropout_prob = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of a model in TensorFlow 2.0 follows the syntax of Keras' `Sequential()` models.\n",
    "\n",
    "Each layer is defined by the `Dense()` function, taking as inputs: the previous layer, the number of nodes, and the activation function (it can actually take a lor of additional arguments, but I'll not review them here). The input layer also requires a definition of the input data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Dense(n_input, input_shape = (n_input,), activation = elu),   # Input layer\n",
    "    \n",
    "    Dense(n_hidden1, activation = elu), # hidden layer 1\n",
    "    Dropout(dropout_prob),     \n",
    "    \n",
    "    Dense(n_hidden2, activation = elu), # hidden layer 2\n",
    "    Dropout(dropout_prob), \n",
    "    \n",
    "    Dense(n_hidden3, activation = elu), # hidden layer 3\n",
    "    Dropout(dropout_prob), \n",
    "    \n",
    "    Dense(n_output, activation = softmax)  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 32        \n",
      "=================================================================\n",
      "Total params: 2,827\n",
      "Trainable params: 2,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of full-Batch Gradient Descent\n",
    "\n",
    "This is Gradient Descent in its simplest form, in which the whole bunch (erhm, batch) of training data is fed into the network at each iteration.\n",
    "\n",
    "(In a following Notebook I will show the implementation of a more powerful technique: **Mini-Batch Gradient Descent**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, I neeed to define a **loss function** (that Gradient Descent will minimize), an **accuracy metrics** (in oreder to monitor the model's performance through the epochs) and an **optimization algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss: Binary cross-entropy is made for \n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Binary Accuracy (expressed in the [0,1] interval)\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Adam Optimizer (what you'll need 99.99% of the time)\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two ways to train a Neural Network in TensorFlow 2.0. The first is using Keras, by calling:\n",
    "\n",
    "    model.compile(optimizer, loss, metrics)\n",
    "\n",
    "The other is to use pure TensorFlow's **eager execution method**, which is what I'll do here. In TensorFlow 1.x what you had to do was to create a computational graph (with placeholders instead of actual data), and then running it in a tf.Session(). Eager execution instead is the TensorFlow's variant of **imperative programming**: operations are evaluated immediately without building graphs, and run instantly. Eager execution makes code is easier to debug, and your whole script much less verbose and easier to read (now it looks pretty much like canonical Python). Another reason why Google's engineers changed TensorFlow so profoudly is that symbolic programming was not so popular (and they want TensorFlow to keep competing against pyTorch, which gained a lot in popularity recently).\n",
    "\n",
    "The reason why I employ eager execution is that I think it gives you more control on model training, and a better control of the output. Also, it forces you to better understand how training a Neural Network works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental element of training a Network in eager execution is represented by `tf.GradientTape()`. This object calculates and stores the gradient of the loss function at each iteration of the training operation. Once you generate a GradientTape, you can call the `.gradient()` argument to get the actual gradient (i.e. the first derivative of the loss function). Later, you feed this values into an `optimizer` using the `.apply_gradients` argument that updates the Network's trainable variables (the very act of \"learning\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.\tTraining Loss: 0.3330511152744293,\tAccuracy: 0.9859155\n",
      "200.\tTraining Loss: 0.24684633314609528,\tAccuracy: 0.9859155\n",
      "300.\tTraining Loss: 0.22913573682308197,\tAccuracy: 0.9859155\n",
      "400.\tTraining Loss: 0.22340820729732513,\tAccuracy: 0.9859155\n",
      "500.\tTraining Loss: 0.22096844017505646,\tAccuracy: 0.9859155\n",
      "600.\tTraining Loss: 0.219722718000412,\tAccuracy: 0.9859155\n",
      "700.\tTraining Loss: 0.2190031260251999,\tAccuracy: 0.9859155\n",
      "800.\tTraining Loss: 0.21854862570762634,\tAccuracy: 0.9859155\n",
      "900.\tTraining Loss: 0.2182389348745346,\tAccuracy: 0.9859155\n",
      "1000.\tTraining Loss: 0.2180042713880539,\tAccuracy: 0.9859155\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "###  TRAINING\n",
    "\n",
    "from pdb import set_trace as bp\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # GratientTape is what I need in order to calculate gradients of the loss function\n",
    "    with tf.GradientTape() as tape:\n",
    "        # take current binary cross-entropy (bce_loss)\n",
    "        current_loss = bce_loss(model(X_train), y_train)\n",
    "    \n",
    "    # HERE THE ACTUAL TRAINING HAPPENS:\n",
    "    # Update weights based on the gradient of the loss function\n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)    # get the gradient of the loss function\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # update the weights\n",
    "    \n",
    "    # save current loss in its history vector\n",
    "    loss_history.append(current_loss.numpy())\n",
    "    \n",
    "    # save current accuracy in its history vector\n",
    "    accuracy.update_state(y_train, model(X_train))  # this computes the accuracy and stores it\n",
    "    current_accuracy = accuracy.result().numpy()  # save its result as numpy object\n",
    "    accuracy_history.append(current_accuracy)\n",
    "    \n",
    "    # In order to monitor progress, I will print loss and accuracy scores every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(str(epoch+1) + '.\\tTraining Loss: ' + str(current_loss.numpy()) + ',\\tAccuracy: ' + str(current_accuracy))\n",
    "    \n",
    "    accuracy.reset_states()  # reset the state of accuracy object for next iteration\n",
    "#\n",
    "print('\\nTraining complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Once the training is done, let's check the model's improvement visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using plt.subplot I can show paired loss and accuracy\n",
    "\n",
    "plt.figure(figsize = (15, 4))  # adjust figures size\n",
    "plt.subplots_adjust(wspace=0.2)  # adjust distance\n",
    "\n",
    "# loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Binary Cross-Entropy')\n",
    "\n",
    "# accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_history)\n",
    "plt.title('Accuracy History')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy scores reached .97+ levels after less than 200 epochs, which is good.\n",
    "\n",
    "At this point, we can make a prediction on the test set, and check the Network's ability to generalize beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)    # (Keras syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs a vector that contains the value of the class with the highest predicted probability\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "# so I do it also for the test data - i.e.: reverse one-hot encoding and get a vector of 0-1 values\n",
    "testdata = np.argmax(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87  1]\n",
      " [ 2 53]]\n"
     ]
    }
   ],
   "source": [
    "# Now I can plot the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(prediction, testdata)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix looks very good: oservations on the matrix diagonal are correct predictions. Luckily, only two datapoints have been misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Accuracy = sum of the diagonal / sum of the whole matrix\n",
    "\n",
    "print('Test Accuracy: ' + str(np.sum(np.diag(CM)) / np.sum(CM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to visualize Confusion Matrices using a heatmap. IMHO, `seaborn` is the best Python library for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e180e3358>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD7pJREFUeJzt3XuQ1eV9x/HPh125uDaKxaEI1hg0EpMKNmoqGjReEi+ZeMmlMbGx1oj5w7TasWrs1EZrJ9pqqZk4pisXoRrA6BgvE7WC10yjSONGieBoqEYYLhJAZeXinvPtH3tkTtmFc9Y9z/7OPrxfmWfC+Z1znvPVYT7z9fk9v9/PESEAQDpDii4AAHJH0AJAYgQtACRG0AJAYgQtACRG0AJAYgQtACRG0AJAYgQtACTWmvoH3l+3nEvP0MOI/T9bdAloQl3bVrq/c/Qlc/YY9bF+/1496GgBILHkHS0ADKhyqegKeiBoAeSl1FV0BT0QtACyElEuuoQeCFoAeSkTtACQFh0tACTGyTAASIyOFgDSCnYdAEBinAwDgMRYOgCAxDgZBgCJ0dECQGKcDAOAxDgZBgBpRbBGCwBpsUYLAImxdAAAidHRAkBipfeLrqAHghZAXlg6AIDEWDoAgMToaAEgMYIWANIKToYBQGKs0QJAYg1aOrB9qKT5VYc+JukaSftIukjSW5XjV0fEz3c1F0ELIC8N6mgj4hVJkyTJdouklZLuk3SBpGkRcVO9cxG0APKS5mTYSZJ+GxFv2O7zl4c0vh4AKFCU6x/1+7qkuVWvL7H9ou2ZtkfW+jJBCyAvXV11D9tTbS+uGlN3nM72UElfkvTTyqHbJI1X97LCKkk31yqJpQMAeelDpxoR7ZLaa3zsNEm/iog1le+s+eAN27dLeqjW7xC0APLS+DXac1W1bGB7TESsqrw8W9KSWhMQtADy0sB9tLbbJJ0i6eKqw/9ie5KkkPT6Du/1iqAFkJcGdrQR0SnpD3c49hd9nYegBZAXrgwDgMS6eNw4AKQVUXQFPRC0APLCbRIBIDGCFgAS42QYACRWKhVdQQ8ELYC8sHQAAIkRtACQGGu0AJBWlNlHCwBpsXQAAImx6wAAEqOj3X3MmXef7n3wEdnWIeM/quuv/ltddOnV6nxvsyRp/YaN+pPDDtUPb7im4EpRlNvbb9YZp5+stW+t06QjTiq6nHw0YdDyzLAE1ry1Tnfdc7/mz/yhfnbnj1Uul/Xwgqc057abdO/sW3Xv7Fs18VOf0EnHTy66VBRozpy7dcYXv1l0GfmJqH8MkJodre0Jks6UNLZyaKWkByJiacrCBruuUklbt25Ta0urNm/Zqv1G7bv9vU2dnVr0q1/r+r+/rMAKUbRnfvGcDjxwXNFl5GewdbS2r5Q0T5IlLaoMS5pr+6r05Q1Oo/cbpb8898s6+Zxv6XNnfkN/0Lanjv3Mp7e/v/DpX+ozn56ovdraCqwSyFQ56h8DpNbSwYWSjoqIGyLizsq4QdLRlffQi7ffeVdPPPOsHv3pLD1+/13avGWrHnz08e3vP7zgKZ1+8gnFFQjkrFSqfwyQWkFblrR/L8fHVN7rVfWz0qfPmbuzj2Xr2cUdGrv/aO07ch/t0dqqk46frI6XXpYkbdj4tl56+RVNmXx0wVUCeYpyue4xUGqt0V4qaaHtVyW9WTn2x5IOlnTJzr5U/az099ctb77LNBIbM3o/vbhkmTZv2aLhw4bpucUd+uSEQyRJ//XEL3T85KM1bNjQgqsEMjXYrgyLiEdsf1zdSwXVJ8Oej4jm2xXcJA7/5ASd8rnj9LULvquWlhZN+Ph4ffXM0yRJDy98St8+72sFV4hmcOd/3qrjpxyjUaP21evLF+va627SrDvmFV3W4NeE9zpwJN7isDt2tKhtxP6fLboENKGubSvd3zk6r/tm3ZnTds1d/f69enDBAoC8dDXff2wTtADy0oRLBwQtgLwMtpNhADDYDOS2rXoRtADyQkcLAIk1YdBy9y4AeWngJbi297F9j+1ltpfaPsb2vrYfs/1q5f9H1pqHoAWQlShH3aMOt0h6JCImSJooaamkqyQtjIhDJC2svN4lghZAXhp09y7be0uaImmGJEXEtojYqO7bxs6ufGy2pLNqlUTQAshLuVz/2LWDJL0laZbtF2xPt90maXRErKp8ZrWk0bUmImgB5KUPHW31nQYrY2rVTK2S/lTSbRFxhKRO7bBMEN33MKi5BsGuAwB56cOug+o7DfZihaQVEfFc5fU96g7aNbbHRMQq22Mkra31O3S0ALISpXLdY5fzRKyW9KbtQyuHTpL0sqQHJJ1fOXa+pPtr1URHCyAvjd1H+11Jd9keKmm5pAvU3aDebftCSW9IqnnfU4IWQFbq3LZV31wRHZKO7OWtPj0fnqAFkJcmvDKMoAWQl+a7pwxBCyAv0dV8SUvQAshL8+UsQQsgL408GdYoBC2AvNDRAkBadLQAkBodLQCkFV1FV9ATQQsgK034tHGCFkBmCFoASIuOFgASI2gBILEouegSeiBoAWSFjhYAEosyHS0AJEVHCwCJRdDRAkBSdLQAkFiZXQcAkBYnwwAgMYIWABKL5rsdLUELIC90tACQGNu7ACCxErsOACAtOloASIw1WgBIrBl3HQwpugAAaKQou+5RD9sttl+w/VDl9R22/9d2R2VMqjUHHS2ArJTKDe8f/0bSUkkfqTr2dxFxT70T0NECyEpE/aMW2+MknSFpen9qImgBZKUcrnvU4d8lXaGez9b9Z9sv2p5me1itSQhaAFmJcN3D9lTbi6vG1A/msf1FSWsj4n92+InvSZog6ShJ+0q6slZNrNECyEpfdh1ERLuk9p28faykL9k+XdJwSR+xfWdEnFd5f6vtWZIur/U7yYO2beyU1D+BQWjlsQcXXQIyVeeSQE0R8T11d6+yfYKkyyPiPNtjImKVbUs6S9KSWnPR0QLISoJdBzu6y/Z+kiypQ9J3an2BoAWQlRTXK0TEk5KerPz5xL5+n6AFkJVGLR00EkELICvcVAYAEmvCh+AStADyEqKjBYCkulg6AIC06GgBIDHWaAEgMTpaAEiMjhYAEivR0QJAWk34bEaCFkBeynS0AJBWEz4El6AFkBdOhgFAYmWzdAAASZWKLqAXBC2ArLDrAAASY9cBACTGrgMASIylAwBIjO1dAJBYiY4WANKiowWAxAhaAEisCR8ZRtACyAsdLQAkxiW4AJAY+2gBILFmXDoYUnQBANBI5T6MXbE93PYi27+2/Rvb11aOH2T7Oduv2Z5ve2itmghaAFmJPowatko6MSImSpok6VTbfybpRknTIuJgSRskXVhrIoIWQFbKrn/sSnTbVHm5R2WEpBMl3VM5PlvSWbVqImgBZKXUh2F7qu3FVWNq9Vy2W2x3SFor6TFJv5W0MSK6Kh9ZIWlsrZo4GQYgK+U+3CgxItolte/i/ZKkSbb3kXSfpAkfpiaCFkBWUuw6iIiNtp+QdIykfWy3VrracZJW1vo+SwcAstKok2G296t0srI9QtIpkpZKekLSVyofO1/S/bVqoqMFkJUGdrRjJM223aLupvTuiHjI9suS5tm+XtILkmbUmoigBZCVLjfmYTYR8aKkI3o5vlzS0X2Zi6AFkBWeGQYAiTXjJbgELYCs9GV710AhaAFkpflilqAFkBmWDgAgsVIT9rQELYCs0NECQGJBRwsAadHR7qbGjRujmTNu0ejRoxQRmj7jJ/rRj2petYdMjZo7T+X3NkvlklQqaf13LlbbBX+lYcceJ0VZ5Q0b9c6NP1D5978vutRBie1du6murpKuuPI6dXQs0V57tem5Zx/WwgVPa+myV4suDQXZcNmlinfe3v76vfnz1DlrpiRpxDlfVtu3zte70/6tqPIGteaLWYJ2QKxevVarV6+VJG3a1Klly17V/mP/iKDFdvHee9v/7OHDmzMtBomuJvyX96GD1vYFETGrkcXsDg48cJwmTvyUFi16oehSUJSQRv7rTZJCmx98UJsfelCS1HbhtzXi819QdG7S+ssuLbbGQawZT4b153601+7sjerHQ5RLnf34iby0te2p+fPadfnl39e7726q/QVkaf1fX6L1F1+kDVdeoRFnnaU9Dj9cktQ5Y7rW/flXtXnBAu159jkFVzl4NeopuI20y6C1/eJOxkuSRu/sexHRHhFHRsSRQ1raGl70YNTa2qr589s1d959+tn9DxddDgpUXrdOkhQbN2rrM89ojwmf+H/vb1nwmIZPmVJEaVmIPvxvoNRaOhgt6QvqfqRuNUv67yQVZar9P27SsmWv6ZZbbi+6FBRp+HDZVmzeLA0frqFHHqXOObPVMnasSiu7n4gy7Njj1PW73xVc6OA1GLd3PSRpr4jo2PEN208mqShDkycfpfPO+4peemmpnl/0qCTpH665UY888njBlWGgtYwcqb3/6XpJkltatGXBAm17fpH2vvY6tR5wgKIcKq9Zo3em3VxwpYNXKZpvjdaRuKihw8Y13z81Cvfm5PFFl4AmNPqJp9zfOb5x4Nl1Z85P3riv379XD7Z3AchKM+46IGgBZGUwrtECwKDCJbgAkBhLBwCQWDPuOiBoAWSFpQMASIyTYQCQGGu0AJAYSwcAkFjqq10/DIIWQFaa8XHj/bkfLQA0nbKi7lGL7Zm219peUnXs+7ZX2u6ojNNrzUPQAshKRNQ96nCHpFN7OT4tIiZVxs9rTcLSAYCsNPJkWEQ8bfuj/Z2HjhZAVvryhIXqx25VxtQ6f+aSytNmZtoeWevDBC2ArJQi6h7Vj92qjPY6fuI2SeMlTZK0SlLNu7SzdAAgK6n30UbEmg/+bPt2dT+JZpcIWgBZSR20tsdExKrKy7MlLdnV5yWCFkBmGnnBgu25kk6QNMr2Ckn/KOkE25MkhaTXJV1cax6CFkBWGrzr4NxeDs/o6zwELYCscFMZAEisFM13o0SCFkBWuKkMACTGbRIBIDHWaAEgsTJLBwCQFh0tACTGrgMASIylAwBIjKUDAEiMjhYAEqOjBYDESlEquoQeCFoAWeESXABIjEtwASAxOloASIxdBwCQGLsOACAxLsEFgMRYowWAxFijBYDE6GgBIDH20QJAYnS0AJAYuw4AIDFOhgFAYiwdAEBiXBkGAInR0QJAYs24RutmTP9c2Z4aEe1F14Hmwt+L/A0puoDdzNSiC0BT4u9F5ghaAEiMoAWAxAjagcU6HHrD34vMcTIMABKjowWAxAjaAWL7VNuv2H7N9lVF14Pi2Z5pe63tJUXXgrQI2gFgu0XSrZJOk3SYpHNtH1ZsVWgCd0g6tegikB5BOzCOlvRaRCyPiG2S5kk6s+CaULCIeFrS+qLrQHoE7cAYK+nNqtcrKscA7AYIWgBIjKAdGCslHVD1elzlGIDdAEE7MJ6XdIjtg2wPlfR1SQ8UXBOAAULQDoCI6JJ0iaRHJS2VdHdE/KbYqlA023Ml/VLSobZX2L6w6JqQBleGAUBidLQAkBhBCwCJEbQAkBhBCwCJEbQAkBhBCwCJEbQAkBhBCwCJ/R9CBfNWGw4R1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.heatmap(CM, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrap the training process in a function\n",
    "\n",
    "Working with Machine (and Deep) Learning requires testing a whole lot of models. Sometimes, writing a block of code for training each Neural Network you want to try might take too much time. For that reason, it becomes useful to define a \"training function\" with parameters you can tune that you can call whenever you need.\n",
    "\n",
    "This is just a wrapper for the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, target, loss, optimizer, metrics, n_epochs, verbose=True, plot=True):\n",
    "    '''\n",
    "    Trains a TensorFlow 2.0 model\n",
    "    \n",
    "    Returns:\n",
    "        prediction: The model's prediction - numpy\n",
    "        loss_history: list\n",
    "        metrics_history: list\n",
    "    \n",
    "    Arguments:\n",
    "        model: a Sequential() model\n",
    "        train: Train dataset (numpy)\n",
    "        target: Target dataset - True outcome (numpy)\n",
    "        loss: loss object or function\n",
    "        optimizer: optimizer object (learnin rate is specified within)\n",
    "        metrics: an evaluation metrics of the model's goodness\n",
    "        n_epochs: number of iterations of training\n",
    "        verbose: print training progress yes/no\n",
    "        plot = plot loss and accuracy histories at the end of training y/n\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    \n",
    "    loss_history = []\n",
    "    metrics_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss = loss(model(train), target)\n",
    "        \n",
    "        gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        loss_history.append(current_loss.numpy())\n",
    "        metrics.update_state(target, model(train))\n",
    "        current_metrics = metrics.result().numpy()\n",
    "        metrics_history.append(current_metrics)\n",
    "        \n",
    "        if verbose:            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(str(epoch+1) \n",
    "                      + '.\\tTraining Loss: ' + str(current_loss.numpy()) \n",
    "                      + ',\\tAccuracy: ' + str(current_accuracy))\n",
    "    \n",
    "        accuracy.reset_states()\n",
    "    \n",
    "    print('\\nTraining complete.')\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize = (15, 4))\n",
    "        plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('Loss History')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Binary Cross-Entropy')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(accuracy_history)\n",
    "        plt.title('Accuracy History')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return prediction, loss_history, metrics_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very bare training function that I could have called as follows:\n",
    "\n",
    "    preds, loss_history, accuracy_history = train_model(model, X_train, y_train, \n",
    "                                                        loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "                                                        optimizer = tf.optimizers.Adam(learning_rate = 0.001), \n",
    "                                                        accuracy = tf.keras.metrics.BinaryAccuracy(), \n",
    "                                                        n_epochs = 1000)\n",
    "\n",
    "With it, you can train this or a similar model with a single line of code. You can also update it with lots of fancier features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is all for now. Thanks for take the time to read my Notebook.\n",
    "\n",
    "Call me crazy, but I think I'm going to miss the good ol' `tf.Session()` of TensorFlow 1.x. However, I must admit that once you've learned the TensorFlow 2.0 eager execution you'll be able to implement Deep Learning model much quicker that usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
