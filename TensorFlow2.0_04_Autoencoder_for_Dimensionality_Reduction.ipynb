{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTY-Msjal7zU"
   },
   "source": [
    "# Autoencoder for Dimensionality Reduction in TensorFlow 2.0\n",
    "\n",
    "### Author: Ivan Bongiorni, Data Scientist at GfK.\n",
    "\n",
    "[LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3M5ezRyrjA3"
   },
   "source": [
    "Summary:\n",
    "\n",
    "\n",
    "1.   What are Autoencoders?\n",
    "2.   Why would you need an Autoencoder?\n",
    "3.   Implementation and Training\n",
    "4.   Extraction of the encoded dataframe\n",
    "5.   Practical application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kg83oWxMm92f"
   },
   "source": [
    "## 1. What are Autoencoders?\n",
    "\n",
    "This tutorial is about a very specific form of Neural Networks: **Autoencoders**.  \n",
    "\n",
    "There are several kinds of Autoencoders around, such as denoising Autoencoders or generative models (variational Autoencoders and GANs); they are all meant to accomplish very specific and different tasks. In this tutorial, I'll focus on their **dimensionality reduction** capabilities. Autoencoders for dimensionality reduction can be conceived as *the Neural Network-equivalent of Principal Component Analysis*.\n",
    "\n",
    "The purpose of this class fo models is neither to learn how to classify observations, nor to predict a continuous output, but to compress, or simplify a dataset. For this reason, they are **unsupervised models**, differently from mainstream Deep Learning models. All the data that pass through an Autoencoder are unlabeled.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpcaQoQ9srG6"
   },
   "source": [
    "### Structure\n",
    "\n",
    "If I had to explain what an Autoencoder is to my grand mother, I'd do it like this: it can be seen as a couple of funnels joined by the tip: in order to make data must flow from one extreme to the other, you must force them through the bottleneck in the middle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvZTU-HzmZEo"
   },
   "source": [
    "More formally, it can be described as follows:\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/800/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n",
    "\n",
    "(image from [towardsdatascience](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAxMjUCBUa43"
   },
   "source": [
    "Each Autoencoder is composed of two parts, an **Encoder** and a **Decoder**. The goal of the Encoder is to \"compress\" the dataset, representing its features with a number of nodes/variables that is narrower than the output layer; the goal of the Decoder is to learn how to reproduce, from the central encoded layer, the initial input values. Autoencoder are thus meant to reproduce the input data on the other extreme as accurately as possible.\n",
    "\n",
    "At this point, you have the right to be puzzled. The first time I've read about this architecture, I was like \"Wait a minute, what's the point of all that?!\". And in fact, it's completely pointless. But reproducing the same dataset on the other side of the Network is a trick that allows for a dimensionality reduction.\n",
    "\n",
    "The fact that the central hidden layers is narrower than the extremes forces the network to find the most efficient way to represent, at that layer, the same data in a more compressed form. The data that flow through that central layer can be seen as a form of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMWIJUSerRVD"
   },
   "source": [
    "## 2. Why would you need an Autoencoder?\n",
    "\n",
    "Why would you need dimensionality reduction?\n",
    "\n",
    "As you have probably learned from PCA, dimensionality reduction techniques are particularly useful when dealing with very complex and/or multicollinear datasets. Thanks to the \"simplification\" they perform, it is also possible to train Machine Learnign models on datasets otherwise too big for your run-down old laptop (joking).\n",
    "\n",
    "### Ok, but why not good ol' PCA?\n",
    "\n",
    "The advantages of this kind of Neural Network architecture over the more classical PCA is that with an Autoencoder you can potentially capture any non-linear pattern of your data. From PCA instead, you can extract factors that are only linearly associated with your actual variables, making the process of dimensionality reduction much more \"rigid\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqK0oiaTsAQn"
   },
   "source": [
    "## 3. Implementation and Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3rPWJBemIEb"
   },
   "source": [
    "### Import data from UCI ML repository\n",
    "\n",
    "The purpose of this dataset is to reduce the dimensionality of the University of Wisconsin's breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOKrNBGSlz2t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", sep=\",\", header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTXgNVxD2VYp"
   },
   "source": [
    "### Dataprep\n",
    "\n",
    "Data preprocessing for Autoencoder require less effort than canonical Neural Networks. First, there is no need to split the data in train and test set, since my goal is just to produce a compressed version of the same dataset. Second, data do not require any labeling; we are in the field of unsupervised Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1NPnVJRbUbe"
   },
   "outputs": [],
   "source": [
    "# get one-hot encoded target variable:\n",
    "target = pd.get_dummies(df.iloc[:,1])\n",
    "target = target.values.astype(np.float32) #convert to float32 for later\n",
    "\n",
    "# I don't need the first two columns:\n",
    "# the first is an id, the other is the target variable ('M'/'B')\n",
    "df = df.iloc[:,2:]\n",
    "df = df.values   # Turn to numpy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MZswZsz2POl"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# no need to use sklearn StandardScaler() this time, but you can if you want\n",
    "def normalize(x):\n",
    "    normalized = (x - np.mean(x))/(np.std(x))\n",
    "    return normalized\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    df[:,i] = normalize(df[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ibv0OMUj7sdH"
   },
   "source": [
    "### Architecture\n",
    "\n",
    "Given the shape of my training dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2590,
     "status": "ok",
     "timestamp": 1550692670707,
     "user": {
      "displayName": "Ivan Bongiorni",
      "photoUrl": "https://lh6.googleusercontent.com/-saQtoUWf1x0/AAAAAAAAAAI/AAAAAAAADl0/slMwgVYQe0w/s64/photo.jpg",
      "userId": "13961768404337863095"
     },
     "user_tz": -60
    },
    "id": "Ac14zUWt-tGG",
    "outputId": "008f1378-74d7-40c0-b6a5-dc6342fcd757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-cA8x_D-6bo"
   },
   "source": [
    "I must now choose an Autoencoder architecture. I want to compress a `(569, 30)` shaped dataset to a `(569, 10)` encoded version. This is an arbitrary choice. In real datascience problems the chioce depends from the nature of your specific dataset (and your specific needs), there is no general rule to determine the right number of encoded variables to be obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qus4X_4XH0V8"
   },
   "outputs": [],
   "source": [
    "# I choose the size of each layer\n",
    "\n",
    "n_input_layer = df.shape[1]\n",
    "n_hidden1 = 30\n",
    "n_hidden2 = 15\n",
    "n_hidden3 = 15\n",
    "\n",
    "n_encoding_layer = 10\n",
    "\n",
    "n_hidden5 = 15\n",
    "n_hidden6 = 15\n",
    "n_hidden7 = 30\n",
    "n_output_layer = n_input_layer  # size of output layer = size of input layer, of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITiY0308HcuD"
   },
   "source": [
    "As you can see, the architecture of the Network si perfectly symmetric. Of course, Encoder and Decoder don't need to be the exact mirror image of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcOyYg3Q8OZY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import elu\n",
    "\n",
    "Autoencoder = tf.keras.models.Sequential([\n",
    "    # ENCODER\n",
    "    Dense(n_input_layer, input_shape = (n_input_layer,), activation = elu),   # Input layer    \n",
    "    Dense(n_hidden1, activation = elu), # hidden layer 1    \n",
    "    Dense(n_hidden2, activation = elu), # hidden layer 2    \n",
    "    Dense(n_hidden3, activation = elu), # hidden layer 3\n",
    "    \n",
    "    # CENTRAL LAYER\n",
    "    Dense(n_encoding_layer, activation = elu, name = 'central_layer'), \n",
    "    \n",
    "    # DECODER\n",
    "    Dense(n_hidden5, activation = elu), # hidden layer 5\n",
    "    Dense(n_hidden6, activation = elu), # hidden layer 6\n",
    "    Dense(n_hidden7, activation = elu), # hidden layer 7\n",
    "    Dense(n_output_layer, activation = elu)  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrgXlzQMWzek"
   },
   "source": [
    "Now that the Network's structure is ready, let me define other hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6iFujQiXZwR"
   },
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "n_epochs = 5000\n",
    "\n",
    "# Loss: mean squared error\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KiGUWCol-uG"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32395,
     "status": "ok",
     "timestamp": 1550692700669,
     "user": {
      "displayName": "Ivan Bongiorni",
      "photoUrl": "https://lh6.googleusercontent.com/-saQtoUWf1x0/AAAAAAAAAAI/AAAAAAAADl0/slMwgVYQe0w/s64/photo.jpg",
      "userId": "13961768404337863095"
     },
     "user_tz": -60
    },
    "id": "Oevz4mHWmCKs",
    "outputId": "0e0cedfe-d45f-4a1d-cc35-78086e7efe47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.\tLoss: 0.2983798086643219\n",
      "400.\tLoss: 0.16014118492603302\n",
      "600.\tLoss: 0.11733168363571167\n",
      "800.\tLoss: 0.09321168065071106\n",
      "1000.\tLoss: 0.07829537242650986\n",
      "1200.\tLoss: 0.07143492251634598\n",
      "1400.\tLoss: 0.06630369275808334\n",
      "1600.\tLoss: 0.06210746988654137\n",
      "1800.\tLoss: 0.059432562440633774\n",
      "2000.\tLoss: 0.057390931993722916\n",
      "2200.\tLoss: 0.05561588332056999\n",
      "2400.\tLoss: 0.05405626446008682\n",
      "2600.\tLoss: 0.052680548280477524\n",
      "2800.\tLoss: 0.05113258957862854\n",
      "3000.\tLoss: 0.04885231703519821\n",
      "3200.\tLoss: 0.04681725054979324\n",
      "3400.\tLoss: 0.045288633555173874\n",
      "3600.\tLoss: 0.04415624216198921\n",
      "3800.\tLoss: 0.04331396520137787\n",
      "4000.\tLoss: 0.042595453560352325\n",
      "4200.\tLoss: 0.0420219786465168\n",
      "4400.\tLoss: 0.04152001813054085\n",
      "4600.\tLoss: 0.04105399176478386\n",
      "4800.\tLoss: 0.04111744463443756\n",
      "5000.\tLoss: 0.04028546065092087\n",
      "\n",
      "Encoding complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_history = []  # save loss improvement\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(Autoencoder(df), df)\n",
    "    \n",
    "    gradients = tape.gradient(current_loss, Autoencoder.trainable_variables)    # get the gradient of the loss function\n",
    "    optimizer.apply_gradients(zip(gradients, Autoencoder.trainable_variables))  # update the weights\n",
    "    \n",
    "    loss_history.append(current_loss.numpy())  # save current loss in its history\n",
    "    \n",
    "    # show loss improvement every 200 epochs\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(str(epoch+1) + '.\\tLoss: ' + str(current_loss.numpy()))\n",
    "#\n",
    "print('\\nEncoding complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xMVYavgX3x6"
   },
   "source": [
    "The computation of `current_loss` deserves attention.\n",
    "\n",
    "As explained above, an Autoencoder doesn't require a dependent variable, i.e. the loss function will not contain any \"test data\". We only need to train the Autoencoder to replicate the input data at the output layer (like comparing the initial dataset with a copy of itself), that's why we are minimizing the distance between the Network's output `Autoencoder(df)` and the actual `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check compression improvements visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32653,
     "status": "ok",
     "timestamp": 1550692700950,
     "user": {
      "displayName": "Ivan Bongiorni",
      "photoUrl": "https://lh6.googleusercontent.com/-saQtoUWf1x0/AAAAAAAAAAI/AAAAAAAADl0/slMwgVYQe0w/s64/photo.jpg",
      "userId": "13961768404337863095"
     },
     "user_tz": -60
    },
    "id": "p2f1s2U2mpqK",
    "outputId": "6c996b63-ca02-47f7-a2f5-32d034773300"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title(\"Autoencoder's training improvement\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction of the encoded dataframe\n",
    "\n",
    "Now that the training of the Autoencoder is done, we can extract our data at the central layer, where they received the strongest compression.\n",
    "\n",
    "In order to do that, we must create an apposite **extraction function** using the **Keras backend submodule**. We use it to implement a Keras function that takes the Autoencoder's input layer as input, and returns the outcome of the central layer as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# create a Keras function\n",
    "extract_encoded_data = K.function(inputs = Autoencoder.layers[0].input, \n",
    "                                  outputs = Autoencoder.layers[4].output)\n",
    "\n",
    "# extract encoded dataframe\n",
    "encoded_dataframe = extract_encoded_data(df)\n",
    "\n",
    "print(encoded_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined `Autoencoder.layers[4]` as the output of that function, since `.layers[4]` is the index of the central layer that compressed my initial dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEDmoV4dfSD4"
   },
   "source": [
    "## 5. Practical application\n",
    "\n",
    "Now that we have an encoded version of our initial dataset, we can use it for our Machine Learning task. Let's implement a Neural Network Classifier and train it on the encoded dataset! Because I want to save time, I'll make it in the most minimal way I know; however, if you need more details, please check [my other tutorials](https://github.com/IvanBongiorni/TensorFlow2.0_Tutorial) in which all of these passages are explained in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_dataframe, target, test_size=0.25, random_state=173)\n",
    "\n",
    "# Scaling of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model architecture - layers: [10, 5, 5, 2]\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import elu, softmax\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Dense(10, input_shape = (X_train.shape[1], ), activation = elu), \n",
    "    Dense(5, activation = elu), \n",
    "    Dense(5, activation = elu),     \n",
    "    Dense(2, activation = softmax)])\n",
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = bce_loss(model(X_train), y_train)    \n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86  1]\n",
      " [ 3 53]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)  # prediction of test data\n",
    "prediction = np.argmax(prediction, axis=1)  # reverse one-hot encoding\n",
    "testdata = np.argmax(y_test, axis=1)  # reverse for target data too\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(prediction, testdata)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "seaborn.heatmap(CM, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: ' + str(np.sum(np.diag(CM)) / np.sum(CM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, I have achieved pretty much the same accuracy that I reached with the [full-Batch model in my first Notebook](https://github.com/IvanBongiorni/TensorFlow2.0_Tutorial/blob/master/TensorFlow2.0_01_basic_Classifier.ipynb)! But this time using a much lighter model trained on a compressed version of the original dataframe.\n",
    "\n",
    "This is the power of Autoencoders. You'll find that computational times will be much slower, and with very similar results. In case of very multicollinear datasets, you might even find a significant improvement in your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: If you want to encode many dataframes, make sure you remember [how to save and restore it](https://github.com/IvanBongiorni/TensorFlow2.0_Tutorial/blob/master/TensorFlow_2.0_03_Save_and_Restore_models.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_3_Autoencoder_Dimensionality_Reduction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
